# Automated-text-tag-classification

This project involves implementing two models for automated tag prediction on Stack Overflow questions using a 10% dataset sample. The initial phase includes developing a baseline tag prediction model with classic classifiers like Logistic Regression and Naive Bayes, followed by a sophisticated model using a language model backbone, such as BERT. The dataset comprises `Questions.csv` and `Tags.csv` files, focusing on questions with positive cumulative scores for high-quality data. Data preprocessing steps include text cleaning, tokenization, and feature extraction using TF-IDF. For the baseline model, classic classifiers are trained and evaluated using metrics like Hamming loss and Jaccard score. The BERT model, fine-tuned on Google Colab, shows superior performance with an accuracy of 0.77 and a Jaccard score of 0.67, outperforming classic classifiers. This project demonstrates that BERT is a promising model for accurately predicting tags, particularly for the most common 15 tags, with potential for further optimization given more resources.
